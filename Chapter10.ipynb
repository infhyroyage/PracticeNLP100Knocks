{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [第10章: ベクトル空間法 (II)](http://www.cl.ecei.tohoku.ac.jp/nlp100/#ch10)\n",
    "第10章では，前章に引き続き単語ベクトルの学習に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. word2vecによる学習\n",
    "81で作成したコーパスに対して[word2vec](https://code.google.com/p/word2vec/)を適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output/Chapter9/81.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Output/Chapter9/81.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284434it [00:02, 111495.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 1.01 s, total: 2min 50s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = None\n",
    "\n",
    "with open(\"Output/Chapter9/81.txt\") as f:\n",
    "    sentences = [line.replace(\"\\n\", \"\").split() for line in tqdm(f)]\n",
    "    model = Word2Vec(sentences=sentences, size=300, min_count=10, seed=90)\n",
    "    model.save(\"Output/Chapter10/90_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"Output/Chapter10/90_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86\n",
    "model.wv[\"United_States\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44872293,  1.6334685 ,  0.42069754,  0.01393384, -0.16998678,\n",
       "        0.02382363,  0.4402721 , -0.0651859 , -0.376896  ,  0.84776735,\n",
       "       -0.4474914 , -0.71033776, -0.40516818, -0.26205367,  0.5669195 ,\n",
       "       -0.8465921 , -0.81093657, -0.3613166 , -0.33018088,  1.2646618 ,\n",
       "        0.08996649,  1.3481137 , -0.0038776 , -0.17100886,  0.3017421 ,\n",
       "        0.01281976, -0.54665107, -0.908616  ,  0.60607755,  0.9918977 ,\n",
       "        1.0866829 ,  0.19219446, -0.9581521 ,  0.3187664 , -1.1826046 ,\n",
       "       -0.513105  , -0.84238786,  0.1298967 ,  0.5220589 ,  0.14251645,\n",
       "       -0.4307814 ,  1.4381793 , -1.3689756 ,  1.2477808 ,  1.2637137 ,\n",
       "        0.5176013 , -0.744362  ,  0.03129834,  1.1492134 ,  0.5294556 ,\n",
       "        0.21340181,  0.7044161 , -0.76726604,  1.4851089 ,  0.5223592 ,\n",
       "        0.729374  , -0.16693877,  0.6762518 ,  2.0744722 ,  1.2172992 ,\n",
       "       -0.6718626 , -0.30356175,  0.38613924,  0.17126812, -0.10497062,\n",
       "        0.6619575 , -0.38637236,  0.18190609, -0.43362013,  0.02566672,\n",
       "       -0.31991807,  0.25134692, -0.40839466, -0.97727126,  0.54358745,\n",
       "        0.5025511 ,  0.55803347,  0.93620926,  0.6369946 , -0.37298074,\n",
       "        1.3569508 ,  0.4250074 , -0.17950135, -1.5515426 , -1.4204973 ,\n",
       "        0.58614486, -0.09036769, -0.0835139 , -0.37437424,  0.8025917 ,\n",
       "       -0.01484932,  0.57827914, -1.1151153 ,  0.6113971 , -1.621311  ,\n",
       "        0.36984366, -1.4763047 , -1.2839824 ,  0.21450165,  0.16465284,\n",
       "        0.6830883 , -0.564068  ,  0.49500957,  0.4780483 , -0.56262136,\n",
       "       -1.2026473 ,  0.62016386, -0.9648239 ,  0.18260008, -0.92419636,\n",
       "        0.5039854 , -0.33115712,  0.31035173,  0.0636378 , -1.0570284 ,\n",
       "       -0.41847008, -0.63177806, -0.988667  , -0.14647096,  0.63994443,\n",
       "        1.3831426 , -0.20532309, -0.6947082 , -0.8978867 ,  0.6050656 ,\n",
       "        0.73572266,  0.23537256, -1.0438501 ,  1.3865651 , -0.23599695,\n",
       "       -0.92872983, -2.712929  ,  0.27820134,  0.05610778,  0.98984116,\n",
       "       -1.7573463 , -1.01968   ,  1.2000886 ,  0.29494825,  1.1172674 ,\n",
       "       -0.4821265 ,  0.60319245, -0.02852156,  0.06084578,  0.58259004,\n",
       "       -1.0608755 ,  0.7369022 ,  0.17107059,  0.6065665 , -0.1197742 ,\n",
       "        0.37623188, -0.46057835,  0.8892176 ,  0.12347095, -0.53480947,\n",
       "       -0.8385887 , -0.75157374,  0.6711949 , -0.83730775, -0.85751224,\n",
       "       -0.71623725,  0.5382629 , -0.50157374, -0.56288016,  0.7505884 ,\n",
       "        0.77208734, -0.5174423 , -0.8250421 , -0.36726862,  0.87253845,\n",
       "       -0.5157334 ,  0.6079125 , -0.5018963 , -0.8571338 ,  0.14448102,\n",
       "       -0.28167978, -0.2172626 , -1.1075728 , -0.18817207,  0.23654535,\n",
       "        0.07909971, -0.87983316,  1.1114815 , -0.2287807 ,  1.8838923 ,\n",
       "        0.32458147,  0.950904  ,  1.0075471 ,  0.34857804, -0.51854044,\n",
       "        0.46063337, -0.38697445, -0.00849148, -1.7454072 , -0.83960056,\n",
       "        0.07787623,  0.6370053 ,  0.01514699, -0.7510575 ,  0.3656621 ,\n",
       "        0.70431167, -0.46270034, -1.3952109 ,  0.40528163, -0.68266344,\n",
       "       -1.2465936 , -1.1408406 , -0.32259744, -0.6059282 ,  0.16294582,\n",
       "        0.52587444, -0.03596535,  0.49109367,  0.24739791,  0.30903393,\n",
       "        0.45084047,  0.48634684,  0.51492554,  0.8304872 , -1.920266  ,\n",
       "        0.9721656 , -0.5978204 , -0.11251215, -0.21063432,  0.66543883,\n",
       "        0.1417977 , -0.08664217,  0.368905  ,  1.1127927 , -0.62961775,\n",
       "       -0.6092499 ,  0.23188174,  0.14667334,  0.31524038,  0.07865784,\n",
       "       -1.5086538 , -0.9264378 , -1.4696659 ,  0.0139272 ,  1.0863217 ,\n",
       "       -1.2972027 ,  0.25493762, -0.2655343 ,  0.5477173 , -1.2151074 ,\n",
       "       -0.42137867, -0.18079919,  1.158064  , -1.8083183 ,  1.0938863 ,\n",
       "        0.86492395,  0.12173139,  0.24477796, -0.4383479 ,  0.4644815 ,\n",
       "        0.76617116,  0.42674157,  0.11911283, -0.03110895, -0.97087145,\n",
       "        0.48989964,  0.19191003, -0.8497676 , -0.2536789 ,  0.4260538 ,\n",
       "       -0.0794576 ,  0.24121256,  0.3505499 ,  0.6028556 , -0.32165772,\n",
       "       -0.08667728,  1.1792488 ,  0.3238023 ,  0.4871153 ,  1.3835008 ,\n",
       "       -0.62960154,  1.7526553 , -0.53819907, -1.1333752 ,  0.25537005,\n",
       "        0.89113516,  0.15062127, -0.8498121 , -0.7760939 , -1.0847154 ,\n",
       "       -0.3018888 , -0.84732956,  0.27870575,  0.48617563, -1.0102859 ,\n",
       "        1.1394439 ,  0.3623804 , -0.19787575, -0.6265708 , -0.38258767,\n",
       "       -0.03887969,  0.93010026,  0.06043873, -1.3424205 , -0.24002303],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86\n",
    "model.wv[\"United_States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83274084"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 87\n",
    "model.wv.n_similarity([\"United_States\"], [\"U.S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Scotland : 0.777856\n",
      "[2/10] Wales : 0.739699\n",
      "[3/10] Ireland : 0.613046\n",
      "[4/10] Britain : 0.608234\n",
      "[5/10] London : 0.574067\n",
      "[6/10] Sweden : 0.563965\n",
      "[7/10] Plymouth : 0.559828\n",
      "[8/10] Birmingham : 0.554329\n",
      "[9/10] Newcastle : 0.548365\n",
      "[10/10] Hampshire : 0.545805\n"
     ]
    }
   ],
   "source": [
    "# 88\n",
    "for i, (word, cos_sim) in enumerate(model.wv.similar_by_word(\"England\", topn=10)):\n",
    "    args = (i+1, word, cos_sim)\n",
    "    print(\"[%d/10] %s : %f\" % args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Spain : 0.885322\n",
      "[2/10] Italy : 0.801836\n",
      "[3/10] Austria : 0.765410\n",
      "[4/10] Denmark : 0.765014\n",
      "[5/10] Egypt : 0.761510\n",
      "[6/10] Russia : 0.761072\n",
      "[7/10] Sweden : 0.746197\n",
      "[8/10] Belgium : 0.743078\n",
      "[9/10] Norway : 0.739321\n",
      "[10/10] Portugal : 0.736502\n"
     ]
    }
   ],
   "source": [
    "# 89\n",
    "x_greece = model.wv[\"Spain\"] - model.wv[\"Madrid\"] + model.wv[\"Athens\"]\n",
    "for i, (word, cos_sim) in enumerate(model.wv.similar_by_vector(x_greece, topn=10)):\n",
    "    args = (i+1, word, cos_sim)\n",
    "    print(\"[%d/10] %s : %f\" % args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. アナロジーデータの準備\n",
    "[単語アナロジーの評価データ](https://raw.githubusercontent.com/svn2github/word2vec/master/questions-words.txt)をダウンロードせよ．このデータ中で\": \"で始まる行はセクション名を表す．例えば，\": capital-common-countries\"という行は，\"capital-common-countries\"というセクションの開始を表している．ダウンロードした評価データの中で，\"family\"というセクションに含まれる評価事例を抜き出してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/questions-words.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Input/questions-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Input/questions-words.txt\") as f, open(\"Output/Chapter10/91.txt\", \"w\") as f_91:\n",
    "    is_family = False\n",
    "    for line in f:\n",
    "        if line == \": family\\n\":\n",
    "            is_family = True\n",
    "            continue\n",
    "        if not is_family:\n",
    "            continue\n",
    "\n",
    "        if line[:2] == \": \":\n",
    "            break\n",
    "        else:\n",
    "            f_91.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. アナロジーデータへの適用\n",
    "91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(t_list)=382481, len(c_list)=382901\n",
      "CPU times: user 195 ms, sys: 49.6 ms, total: 245 ms\n",
      "Wall time: 966 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_list, c_list = None, None\n",
    "\n",
    "with open(\"Output/Chapter9/83_f_t.txt\") as f_83_t, open(\"Output/Chapter9/83_f_c.txt\") as f_83_c:\n",
    "    t_list = [line.split()[1] for line in f_83_t]\n",
    "    c_list = [line.split()[1] for line in f_83_c]\n",
    "\n",
    "print(\"len(t_list)=%d, len(c_list)=%d\" % (len(t_list), len(c_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382481, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_85 = None\n",
    "\n",
    "with open(\"Output/Chapter9/85_X_comp.pickle\", \"rb\") as f_85:\n",
    "    X_85 = pickle.load(f_85)\n",
    "\n",
    "X_85.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim_for_X_85(x: np.ndarray, y: np.ndarray):\n",
    "    if np.linalg.norm(x) == 0 or np.linalg.norm(y) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506it [27:06,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# 85\n",
    "with open(\"Output/Chapter10/91.txt\") as f_91, open(\"Output/Chapter10/92_from_85.txt\", \"w\") as f_92:\n",
    "    for line in tqdm(f_91):\n",
    "        word, cossim, elements = \"???\", -1.0, line.replace(\"\\n\", \"\").split()\n",
    "\n",
    "        # 91.txtの1~3列目の単語のいずれかが、コーパスに存在しない場合は、\n",
    "        # ベクトルが算出できないため、類似度が最も高い単語は\"???\"、その類似度は-1.0として出力\n",
    "        if elements[0] in t_list and elements[1] in t_list and elements[2] in t_list:\n",
    "            indices = (t_list.index(elements[0]), t_list.index(elements[1]), t_list.index(elements[2]))\n",
    "            x = X_85[indices[1]] - X_85[indices[0]] + X_85[indices[2]]\n",
    "            cossims = [cossim_for_X_85(x, X_85[i]) for i in range(len(t_list))]\n",
    "            idx = np.argmax(cossims)\n",
    "            word, cossim = t_list[idx], cossims[idx]\n",
    "\n",
    "        args = (line.replace(\"\\n\", \"\"), word, cossim)\n",
    "        f_92.write(\"%s %s %f\\n\" % args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506it [00:01, 334.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 90\n",
    "with open(\"Output/Chapter10/91.txt\") as f_91, open(\"Output/Chapter10/92_from_90.txt\", \"w\") as f_92:\n",
    "    for line in tqdm(f_91):\n",
    "        word, cossim, elements = \"???\", -1.0, line.replace(\"\\n\", \"\").split()\n",
    "\n",
    "        # 91.txtの1~3列目の単語のいずれかが、コーパスに存在しない場合は、\n",
    "        # ベクトルが算出できないため、類似度が最も高い単語は\"???\"、その類似度は-1.0として出力\n",
    "        if elements[0] in model.wv and elements[1] in model.wv and elements[2] in model.wv:\n",
    "            x = model.wv[elements[1]] - model.wv[elements[0]] + model.wv[elements[2]]\n",
    "            word, cossim = model.wv.similar_by_vector(x, topn=1)[0]\n",
    "\n",
    "        args = (line.replace(\"\\n\", \"\"), word, cossim)\n",
    "        f_92.write(\"%s %s %f\\n\" % args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. アナロジータスクの正解率の計算\n",
    "92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Model:1.778656%, 90 Model:9.683794%\n"
     ]
    }
   ],
   "source": [
    "with open(\"Output/Chapter10/92_from_85.txt\") as f_from_85, open(\"Output/Chapter10/92_from_90.txt\") as f_from_90:\n",
    "    accuracies = []\n",
    "\n",
    "    for f in [f_from_85, f_from_90]:\n",
    "        total, correct = 0, 0\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            elements = line.replace(\"\\n\", \"\").split()\n",
    "            if elements[3] == elements[4]:\n",
    "                correct += 1\n",
    "        accuracies.append(100.0 * correct / total)\n",
    "\n",
    "    print(\"85 Model:%f%%, 90 Model:%f%%\" % (accuracies[0], accuracies[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. WordSimilarity-353での類似度計算\n",
    "[The WordSimilarity-353 Test Collection](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)の評価データを入力とし，1列目と2列目の単語の類似度を計算し，各行の末尾に類似度の値を追加するプログラムを作成せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "2020年4月19日現在、[The WordSimilarity-353 Test Collection](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)の評価データのリンクが切れているため、[WordSim353 - Similarity and Relatednessを用いて何らかの評価を行った結果のデータ](http://alfonseca.org/pubs/ws353simrel.tar.gz)の中からwordsim353_agreed.txtを借用し、それを成形することで代用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/wordsim353_agreed.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Input/wordsim353_agreed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Input/wordsim353_agreed.txt\") as f, open(\"Output/Chapter10/94_wordsim-353.txt\", \"w\") as f_94:\n",
    "    for line in f:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elements = line.split()\n",
    "        f_94.write(\"%s\\t%s\\t%s\\n\" % (elements[1], elements[2], elements[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output/Chapter10/94_wordsim-353.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Output/Chapter10/94_wordsim-353.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "352it [00:04, 77.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 85\n",
    "with open(\"Output/Chapter10/94_wordsim-353.txt\") as f, open(\"Output/Chapter10/94_from_85.txt\", \"w\") as f_94:\n",
    "    for line in tqdm(f):\n",
    "        cossim, elements = -1.0, line.replace(\"\\n\", \"\").split()\n",
    "\n",
    "        # 94_wordsim-353.txtの1~2列目の単語のいずれかが、コーパスに存在しない場合は、\n",
    "        # ベクトルが算出できないため、類似度を-1.0として出力\n",
    "        if elements[0] in t_list and elements[1] in t_list:\n",
    "            indices = (t_list.index(elements[0]), t_list.index(elements[1]))\n",
    "            cossim = cossim_for_X_85(X_85[indices[0]], X_85[indices[1]])\n",
    "\n",
    "        args = (line.replace(\"\\n\", \"\"), cossim)\n",
    "        f_94.write(\"%s %f\\n\" % args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "352it [00:00, 22607.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 90\n",
    "with open(\"Output/Chapter10/94_wordsim-353.txt\") as f, open(\"Output/Chapter10/94_from_90.txt\", \"w\") as f_94:\n",
    "    for line in tqdm(f):\n",
    "        cossim, elements = -1.0, line.replace(\"\\n\", \"\").split()\n",
    "\n",
    "        # 94_wordsim-353.txtの1~2列目の単語のいずれかが、コーパスに存在しない場合は、\n",
    "        # ベクトルが算出できないため、類似度を-1.0として出力\n",
    "        if elements[0] in model.wv and elements[1] in model.wv:\n",
    "            cossim = model.wv.n_similarity([elements[0]], [elements[1]])\n",
    "\n",
    "        args = (line.replace(\"\\n\", \"\"), cossim)\n",
    "        f_94.write(\"%s %f\\n\" % args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. WordSimilarity-353での評価\n",
    "94で作ったデータを用い，各モデルが出力する類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Model:0.090700%, 90 Model:0.493397%\n"
     ]
    }
   ],
   "source": [
    "with open(\"Output/Chapter10/94_from_85.txt\") as f_from_85, open(\"Output/Chapter10/94_from_90.txt\") as f_from_90:\n",
    "    correlations = []\n",
    "\n",
    "    for f in [f_from_85, f_from_90]:\n",
    "        cossims, humans = [], []\n",
    "        for line in f:\n",
    "            elements = line.replace(\"\\n\", \"\").split()\n",
    "            cossims.append(float(elements[3]))\n",
    "            humans.append(float(elements[2]))\n",
    "\n",
    "        correlations.append(spearmanr(cossims, humans)[0])\n",
    "\n",
    "    print(\"85 Model:%f%%, 90 Model:%f%%\" % (correlations[0], correlations[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 国名に関するベクトルの抽出\n",
    "word2vecの学習結果から，国名に関するベクトルのみを抜き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. k-meansクラスタリング\n",
    "96の単語ベクトルに対して，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. Ward法によるクラスタリング\n",
    "96の単語ベクトルに対して，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. t-SNEによる可視化\n",
    "96の単語ベクトルに対して，ベクトル空間をt-SNEで可視化せよ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
