{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [第8章: 機械学習](http://www.cl.ecei.tohoku.ac.jp/nlp100/#ch8)\n",
    "本章では，Bo Pang氏とLillian Lee氏が公開している[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/)の[sentence polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. データの入手・整形\n",
    "[文に関する極性分析の正解データ](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)を用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "1. rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "2. rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "3. 上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Corpus/rt-polarity.neg  ../Corpus/rt-polarity.pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Corpus/rt-polarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Corpus/rt-polarity.pos\", \"rb\") as pos, open(\"../Corpus/rt-polarity.neg\", \"rb\") as neg:\n",
    "    sentiment_lines = []\n",
    "    sentiment_lines.extend([b\"+1 \" + pos_line for pos_line in pos])\n",
    "    sentiment_lines.extend([b\"-1 \" + neg_line for neg_line in neg])\n",
    "    random.shuffle(sentiment_lines)\n",
    "    with open(\"rt-polarity.sentiment\", \"wb\") as sentiment:\n",
    "        for sentiment_line in sentiment_lines:\n",
    "            sentiment.write(sentiment_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 passions , obsessions , and loneliest dark spots are pushed to their most virtuous limits , lending the narrative an unusually surreal tone . \r\n",
      "+1 it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . i haven't seen one in so long , no wonder i didn't recognize it at first . \r\n",
      "-1 the film takes too long getting to the good stuff , then takes too long figuring out what to do next . \r\n",
      "+1 leguizamo and jones are both excellent and the rest of the cast is uniformly superb . \r\n",
      "+1 if s&m seems like a strange route to true love , maybe it is , but it's to this film's ( and its makers' ) credit that we believe that that's exactly what these two people need to find each other -- and themselves . \r\n",
      "-1 a mediocre exercise in target demographics , unaware that it's the butt of its own joke . \r\n",
      "+1 if you thought tom hanks was just an ordinary big-screen star , wait until you've seen him eight stories tall . \r\n",
      "-1 the overall feel is not unlike watching a glorified episode of \" 7th heaven . \" \r\n",
      "-1 it's so crammed with scenes and vistas and pretty moments that it's left a few crucial things out , like character development and coherence . \r\n",
      "+1 remarkable for its excellent storytelling , its economical , compressed characterisations and for its profound humanity , it's an adventure story and history lesson all in one . \r\n"
     ]
    }
   ],
   "source": [
    "!head -10 rt-polarity.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. ストップワード\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['towards',\n",
       " 'up',\n",
       " 'both',\n",
       " 'behind',\n",
       " 'nothing',\n",
       " 'through',\n",
       " 'go',\n",
       " 'anyone',\n",
       " 'beside',\n",
       " 'onto']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop_words\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stop_words(word: str):\n",
    "    return word in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we : True\n",
      "nlp : False\n"
     ]
    }
   ],
   "source": [
    "print(\"we : %s\" % check_stop_words(\"we\"))\n",
    "print(\"nlp : %s\" % check_stop_words(\"nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 素性抽出\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "lines = []\n",
    "features = []\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# 文字化けによるエンコーディングエラー時は、その文字を無視\n",
    "with open(\"rt-polarity.sentiment\", errors=\"ignore\") as sentiment:\n",
    "    for sentiment_line in sentiment:\n",
    "        label, line, stem_words = int(sentiment_line[0:2]), sentiment_line[3:], []\n",
    "\n",
    "        for word in line.split():\n",
    "            if not check_stop_words(word):\n",
    "                stem_words.append(porter.stem(word))\n",
    "        if len(stem_words) == 0:\n",
    "            continue\n",
    "\n",
    "        labels.append(label)\n",
    "        lines.append(line)\n",
    "        features.append(\" \".join(stem_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passion , obsess , loneliest dark spot push virtuou limit , lend narr unusu surreal tone .',\n",
       " \"kickass , dens sci-fi action thriller hybrid deliv . haven't seen long , wonder didn't recogn .\",\n",
       " 'film take long get good stuff , take long figur .',\n",
       " 'leguizamo jone excel rest cast uniformli superb .',\n",
       " \"s&m like strang rout true love , mayb , it' film' ( makers' ) credit believ that' exactli peopl need -- .\",\n",
       " \"mediocr exercis target demograph , unawar it' butt joke .\",\n",
       " \"thought tom hank just ordinari big-screen star , wait you'v seen stori tall .\",\n",
       " 'overal feel unlik watch glorifi episod \" 7th heaven . \"',\n",
       " \"it' cram scene vista pretti moment it' left crucial thing , like charact develop coher .\",\n",
       " \"remark excel storytel , econom , compress characteris profound human , it' adventur stori histori lesson .\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, -1, 1, 1, -1, 1, -1, -1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 学習\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10662x13296 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 106372 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ここでは、tf-idfによるベクトル化により、コーパスから学習データを作成\n",
    "tf_idf_73 = TfidfVectorizer().fit(features)\n",
    "X_73 = tf_idf_73.transform(features)\n",
    "X_73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_73 = np.array(labels)\n",
    "y_73.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=73, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_73 = LogisticRegression(random_state=73)\n",
    "model_73.fit(X_73, y_73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881541924592009"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_73.score(X_73, y_73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 予測\n",
    "73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら\"+1\"，負例なら\"-1\"）と，その予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passions , obsessions , and loneliest dark spots are pushed to their most virtuous limits , lending the narrative an unusually surreal tone . \n",
      "-> Label:+1, Predict:+1, Probability:0.687160\n",
      "\n",
      "it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . i haven't seen one in so long , no wonder i didn't recognize it at first . \n",
      "-> Label:+1, Predict:-1, Probability:0.526148\n",
      "\n",
      "the film takes too long getting to the good stuff , then takes too long figuring out what to do next . \n",
      "-> Label:-1, Predict:-1, Probability:0.561616\n",
      "\n",
      "leguizamo and jones are both excellent and the rest of the cast is uniformly superb . \n",
      "-> Label:+1, Predict:+1, Probability:0.618260\n",
      "\n",
      "if s&m seems like a strange route to true love , maybe it is , but it's to this film's ( and its makers' ) credit that we believe that that's exactly what these two people need to find each other -- and themselves . \n",
      "-> Label:+1, Predict:+1, Probability:0.688766\n",
      "\n",
      "a mediocre exercise in target demographics , unaware that it's the butt of its own joke . \n",
      "-> Label:-1, Predict:-1, Probability:0.935489\n",
      "\n",
      "if you thought tom hanks was just an ordinary big-screen star , wait until you've seen him eight stories tall . \n",
      "-> Label:+1, Predict:+1, Probability:0.589269\n",
      "\n",
      "the overall feel is not unlike watching a glorified episode of \" 7th heaven . \" \n",
      "-> Label:-1, Predict:-1, Probability:0.761696\n",
      "\n",
      "it's so crammed with scenes and vistas and pretty moments that it's left a few crucial things out , like character development and coherence . \n",
      "-> Label:-1, Predict:-1, Probability:0.778911\n",
      "\n",
      "remarkable for its excellent storytelling , its economical , compressed characterisations and for its profound humanity , it's an adventure story and history lesson all in one . \n",
      "-> Label:+1, Predict:+1, Probability:0.890254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for i in range(len(lines)):\n",
    "for i in range(10):\n",
    "    line = lines[i]\n",
    "    label = \"-1\" if labels[i] == -1 else \"+1\"\n",
    "    pred = \"-1\" if model_73.predict(X_73[i]) == -1 else \"+1\"\n",
    "    probs = model_73.predict_proba(X_73[i])[0]\n",
    "    prob = probs[0] if pred == \"-1\" else probs[1]\n",
    "\n",
    "    args = (line, label, pred, prob)\n",
    "    print(\"%s-> Label:%s, Predict:%s, Probability:%f\" % args)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 素性の重み\n",
    "73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13296,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(model_73.coef_[0])\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Best 10 Features]\n",
      "No.1\t: beauti\t(coef=3.088591)\n",
      "No.2\t: perform\t(coef=2.529640)\n",
      "No.3\t: enjoy\t(coef=2.452630)\n",
      "No.4\t: solid\t(coef=2.319076)\n",
      "No.5\t: entertain\t(coef=2.318588)\n",
      "No.6\t: heart\t(coef=2.315182)\n",
      "No.7\t: fun\t(coef=2.315047)\n",
      "No.8\t: best\t(coef=2.280014)\n",
      "No.9\t: cinema\t(coef=2.245044)\n",
      "No.10\t: engross\t(coef=2.237097)\n"
     ]
    }
   ],
   "source": [
    "print(\"[Best 10 Features]\")\n",
    "for i in range(-1, -11, -1):\n",
    "    best_idx = indices[i]\n",
    "    best_feature = [k for k, v in tf_idf_73.vocabulary_.items() if v == best_idx]\n",
    "\n",
    "    args = (-i, best_feature[0], model_73.coef_[0][best_idx])\n",
    "    print(\"No.%s\\t: %s\\t(coef=%f)\" % args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worst 10 Features]\n",
      "No.1\t: bore\t(coef=-3.396291)\n",
      "No.2\t: bad\t(coef=-3.219395)\n",
      "No.3\t: dull\t(coef=-3.081518)\n",
      "No.4\t: lack\t(coef=-2.600852)\n",
      "No.5\t: worst\t(coef=-2.562439)\n",
      "No.6\t: fail\t(coef=-2.535151)\n",
      "No.7\t: joke\t(coef=-2.249139)\n",
      "No.8\t: thing\t(coef=-2.115947)\n",
      "No.9\t: mediocr\t(coef=-2.111887)\n",
      "No.10\t: wast\t(coef=-2.092020)\n"
     ]
    }
   ],
   "source": [
    "print(\"[Worst 10 Features]\")\n",
    "for i in range(10):\n",
    "    worst_idx = indices[i]\n",
    "    worst_feature = [k for k, v in tf_idf_73.vocabulary_.items() if v == worst_idx]\n",
    "\n",
    "    args = (i+1, worst_feature[0], model_73.coef_[0][worst_idx])\n",
    "    print(\"No.%s\\t: %s\\t(coef=%f)\" % args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. ラベル付け\n",
    "学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. 正解率の計測\n",
    "76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. 5分割交差検定\n",
    "76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 適合率-再現率グラフの描画\n",
    "ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
